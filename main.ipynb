{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:14.306225Z",
     "start_time": "2024-07-22T22:39:09.188885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import string\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e80e639fdfb613",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:14.552735Z",
     "start_time": "2024-07-22T22:39:14.310306Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50e1430210855f5",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74374ca81b905340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:14.660674Z",
     "start_time": "2024-07-22T22:39:14.556186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the csv\n",
    "df = pd.read_csv('100.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d9858874fbe84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:14.675811Z",
     "start_time": "2024-07-22T22:39:14.663734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Category count\n",
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75ba4b43a1483a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:14.707653Z",
     "start_time": "2024-07-22T22:39:14.678814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns that are not needed for the analysis.\n",
    "df.drop(columns=['Unnamed: 0','link','date','authors'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b85353c3c4bcc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:14.753305Z",
     "start_time": "2024-07-22T22:39:14.710755Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merging headline and short description.\n",
    "df['text'] = df['headline'] + \" \" + df['short_description']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e25a4d11a3881e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:14.783924Z",
     "start_time": "2024-07-22T22:39:14.755445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping columns after merging the important columns.\n",
    "df = df.drop(columns=['headline', 'short_description'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee310ce7bac8cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:14.814847Z",
     "start_time": "2024-07-22T22:39:14.792121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sum of the NA values in each category\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace1b3a1091f6466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:14.830113Z",
     "start_time": "2024-07-22T22:39:14.817947Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ccad0b8e341fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:14.846236Z",
     "start_time": "2024-07-22T22:39:14.833718Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove digits (optional)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef064deea32cd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:22.188416Z",
     "start_time": "2024-07-22T22:39:14.849328Z"
    }
   },
   "outputs": [],
   "source": [
    "# After removing lowercase, punctuations and stop words.\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8999cc01e739c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:22.203690Z",
     "start_time": "2024-07-22T22:39:22.191390Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ba1595f4a9344",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:22.218893Z",
     "start_time": "2024-07-22T22:39:22.206683Z"
    }
   },
   "outputs": [],
   "source": [
    "# Common words in Category.\n",
    "def get_most_common_terms(category, n=20):\n",
    "    vectorizer = CountVectorizer(max_features=1000)\n",
    "    category_texts = df[df['category'] == category]['text']\n",
    "    X = vectorizer.fit_transform(category_texts)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    sums = X.sum(axis=0)\n",
    "    term_freq = [(term, sums[0, idx]) for term, idx in vectorizer.vocabulary_.items()]\n",
    "    term_freq = sorted(term_freq, key=lambda x: x[1], reverse=True)\n",
    "    return term_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dbc424c3fc634d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:22.449997Z",
     "start_time": "2024-07-22T22:39:22.221895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Most Common terms in each of the categories.\n",
    "categories = df['category'].unique()\n",
    "for category in categories:\n",
    "    print(f\"\\nMost common terms in category '{category}':\")\n",
    "    common_terms = get_most_common_terms(category)\n",
    "    for term, freq in common_terms:\n",
    "        print(f\"{term}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844b69839e017ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:22.480618Z",
     "start_time": "2024-07-22T22:39:22.453092Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sentence length analysis\n",
    "df['sentence_length'] = df['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0ac3b5d0f8c87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:22.965890Z",
     "start_time": "2024-07-22T22:39:22.482672Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='category', y='sentence_length', data=df)\n",
    "plt.title('Sentence Length Distribution by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Sentence Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee8ffa4bf81168",
   "metadata": {},
   "source": [
    "\n",
    "Both the POLITICS and STYLE categories have a similar median sentence length of around 20 words. However, the POLITICS category exhibits more significant outliers and longer sentences, with the maximum sentence length approaching 140 words, Compared to around 100 words for the STYLE category. This indicates that POLITICS articles tend to include longer sentences, while STYLE articles are relatively more concise. The overall distribution shows that sentence lengths for both categories are concentrated around the median, But the POLITICS category has a longer tail, indicating more long sentences.\n",
    "\n",
    "The most common words ：\n",
    "POLITICS： mainly focused on political figures (such as Trump, Clinton) and related political terms (such as President, GOP).\n",
    "STYLE： mainly concentrated on fashion and beauty related terms (such as fashion, style, appearance, beauty).\n",
    "\n",
    "Category Style has less data points than category Politics that might favor category Politics more. We can oversample the category Style or we can apply more weight to the Style category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438d50004b79491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:22.981223Z",
     "start_time": "2024-07-22T22:39:22.968938Z"
    }
   },
   "outputs": [],
   "source": [
    "# To remove the outliers using IQR\n",
    "def categoryOut(cate):\n",
    "    Q1 = df[df['category']==cate]['sentence_length'].quantile(0.25)\n",
    "    Q3 = df[df['category']==cate]['sentence_length'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5*IQR\n",
    "    upper = Q3 + 1.5*IQR\n",
    "    return lower,upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9057df39f8a6bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:23.011874Z",
     "start_time": "2024-07-22T22:39:22.983251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lower and Upper limit of Politics. Points outside this range should be considered as outliers.\n",
    "lp,up=categoryOut('POLITICS')\n",
    "lp,up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55344b953f1db35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:23.043231Z",
     "start_time": "2024-07-22T22:39:23.014904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lower and Upper limit of Style. Points outside this range should be considered as outliers.\n",
    "lc,uc=categoryOut('STYLE')\n",
    "lc,uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ab9ca5d4c7716",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:23.058508Z",
     "start_time": "2024-07-22T22:39:23.045256Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[\n",
    "    ( (df['category'] == 'POLITICS') & (df['sentence_length'] >= lp) & (df['sentence_length'] <= up))\n",
    "    |\n",
    "    ( (df['category'] == 'STYLE') & (df['sentence_length'] >= lc) & (df['sentence_length'] <= uc))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ddbb15854fe68d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:23.073728Z",
     "start_time": "2024-07-22T22:39:23.060555Z"
    }
   },
   "outputs": [],
   "source": [
    "# Outliers removed\n",
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fccfd0bc950214",
   "metadata": {},
   "source": [
    "Category Style has less data points than category Politics that might favor category Politics more. We can oversample the category Style or we can apply more weight to the Style category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca79ab80c14814",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9002fc33bde7b5b1",
   "metadata": {},
   "source": [
    "The split strategy that I chose is a good practice for creating a reliable and balanced dataset for training, validation, and testing purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923bb5e86b995674",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:23.104257Z",
     "start_time": "2024-07-22T22:39:23.076807Z"
    }
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset into training, validation and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['category'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f52a6436d88e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:23.165578Z",
     "start_time": "2024-07-22T22:39:23.108360Z"
    }
   },
   "outputs": [],
   "source": [
    "#Saving the train,validation and test locally.\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "val_df.to_csv('valid.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82150ead28b7bfa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:23.243512Z",
     "start_time": "2024-07-22T22:39:23.167578Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "valid_df = pd.read_csv('valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27ac98bc44864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:23.274888Z",
     "start_time": "2024-07-22T22:39:23.247572Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e740ff27494b73c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:23.305841Z",
     "start_time": "2024-07-22T22:39:23.291500Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea516469c3885e1a",
   "metadata": {},
   "source": [
    "In this step, we use the TF-IDF Vectorizer to convert the text data into numerical representations. TF-IDF helps to highlight important words in each document while reducing the weight of commonly occurring words that are less informative (like \"the\", \"is\", \"in\", etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e4a2b7df843e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:23.508131Z",
     "start_time": "2024-07-22T22:39:23.310253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting words to numerical representation\n",
    "vectorizer = TfidfVectorizer(max_features=4500, stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "\n",
    "# Transform the validation data\n",
    "X_valid = vectorizer.transform(valid_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ba7b446db35976",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:23.523376Z",
     "start_time": "2024-07-22T22:39:23.510187Z"
    }
   },
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y_train = train_df['category']\n",
    "y_valid = valid_df['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521383ae6f2dcd44",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Choice of Classifier: Logistic Regression is a simple and interpretable model suitable for binary classification problems. It performs well with a large number of features and provides probabilistic outputs.\n",
    "Parameters Used: We used the default parameters except for setting random_state=42 to ensure reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9228ecb8d11ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:23.585021Z",
     "start_time": "2024-07-22T22:39:23.526343Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Training the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#Saving the model\n",
    "joblib.dump(logreg, 'logreg_model_without_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f53cba6aa1636",
   "metadata": {},
   "source": [
    "\n",
    "### Random Forest Classifier\n",
    "Choice of Classifier: Random Forest is an ensemble learning method that constructs multiple decision trees and merges their results. It tends to provide high accuracy and robustness to overfitting, making it a good choice for complex datasets.\n",
    "Parameters Used: We set random_state=42 for reproducibility and used n_estimators=100 to specify the number of trees in the forest, which is a common choice to balance performance and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec16ab4d97bc3cca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:27.553704Z",
     "start_time": "2024-07-22T22:39:23.588090Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "\n",
    "# Training the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#Saving the model\n",
    "joblib.dump(rf, 'random_forest_without_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac6d7bf58a7dd5",
   "metadata": {},
   "source": [
    "### Deep learning model\n",
    "##### Multi Layer perceptron (MLP)\n",
    "When text data is converted into numerical vectors using techniques like TF-IDF, the resulting feature vectors can be effectively handled by MLPs. These vectorized features represent the text in a format that MLPs are well-equipped to process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5865e282a8c1a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:27.568903Z",
     "start_time": "2024-07-22T22:39:27.555705Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_dl = train_df.copy()\n",
    "valid_df_dl = val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518315c4a482dd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:27.600152Z",
     "start_time": "2024-07-22T22:39:27.575958Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_df_dl['category'] = label_encoder.fit_transform(train_df_dl['category'])\n",
    "valid_df_dl['category'] = label_encoder.transform(valid_df_dl['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c729f824b319f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:27.616252Z",
     "start_time": "2024-07-22T22:39:27.603150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y_train_dl = train_df_dl['category'].values\n",
    "y_valid_dl = valid_df_dl['category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487227509364efb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:27.876271Z",
     "start_time": "2024-07-22T22:39:27.619278Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=4500, stop_words='english')\n",
    "\n",
    "# Fitting and transforming the training data\n",
    "X_train_dl = vectorizer.fit_transform(train_df_dl['text']).toarray()\n",
    "\n",
    "# Transforming the validation data\n",
    "X_valid_dl = vectorizer.transform(valid_df_dl['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499984b480f8cac6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:27.937068Z",
     "start_time": "2024-07-22T22:39:27.879360Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_dl, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_dl, dtype=torch.long)\n",
    "X_valid_tensor = torch.tensor(X_valid_dl, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid_dl, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c10da78244698e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:27.952203Z",
     "start_time": "2024-07-22T22:39:27.940069Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd5f9acd160a348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:27.968060Z",
     "start_time": "2024-07-22T22:39:27.956205Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557eefedda981c9",
   "metadata": {},
   "source": [
    "The MLP (Multilayer Perceptron) architecture consists of an input layer connected to a hidden layer via a linear transformation, followed by a ReLU activation function. The hidden layer is then connected to the output layer, which performs a linear transformation to produce the final class scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8962c182c7624f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:27.984119Z",
     "start_time": "2024-07-22T22:39:27.972060Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb63fb12733dac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:27.999422Z",
     "start_time": "2024-07-22T22:39:27.986601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the model, loss function, and optimizer\n",
    "input_size = X_train_dl.shape[1]\n",
    "hidden_size = 100\n",
    "num_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab32be18f25f4b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:30.179745Z",
     "start_time": "2024-07-22T22:39:28.003432Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3a0f8596ceb9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:30.195471Z",
     "start_time": "2024-07-22T22:39:30.179745Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mlp_model_state.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390d9ae9db01e5e",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6c5b51832c761",
   "metadata": {},
   "source": [
    "Since our dataset is imbalanced, with more political data samples than style data samples, the F1-score is the ideal choice for evaluating our binary classification models. The F1-score is the harmonic mean of precision and recall, providing a single measure that balances both metrics.\n",
    "\n",
    "Balance Between Precision and Recall: Precision measures how many of the selected items are relevant, while recall measures how many relevant items are selected. The F1-score combines these into one metric, ensuring that neither precision nor recall is unduly prioritized.\n",
    "Handling Imbalanced Data: In imbalanced datasets, where one class dominates, accuracy can be misleading because the model could achieve high accuracy by simply predicting the majority class. The F1-score, by considering both false positives and false negatives, provides a more nuanced and informative evaluation.\n",
    "Relevance to Our Task: For distinguishing between political and style news categories, it is crucial to minimize both false positives and false negatives. The F1-score effectively captures this by taking into account both types of errors.\n",
    "\n",
    "Good Benchmark\n",
    "For many binary classification tasks, an F1-score above 0.70 is considered good, indicating a reasonable balance between precision and recall. An F1-score close to 0.90 is considered excellent, demonstrating that the model performs very well on both the majority and minority classes. These benchmarks can guide us in evaluating the performance of our models:\n",
    "\n",
    "Above 0.70: A good performance indicator, showing the model is reasonably balanced between precision and recall. \n",
    "Close to 0.90: Indicates excellent performance, with the model effectively handling both political and style categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff28b0151f8713",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:30.211206Z",
     "start_time": "2024-07-22T22:39:30.195471Z"
    }
   },
   "outputs": [],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58393c00e6ef37b",
   "metadata": {},
   "source": [
    "#### Performance evaluation of Logistic regression\n",
    "\n",
    "We will use the F1 score as our primary metric for evaluation. Additionally, we will leverage the classification report to gain deeper insights into precision and recall for specific categories, such as \"political\" and \"style.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12b0a4d09d75a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:30.243102Z",
     "start_time": "2024-07-22T22:39:30.211206Z"
    }
   },
   "outputs": [],
   "source": [
    "logistic_regression_model = joblib.load('logreg_model_without_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8be24f833c566",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:30.384114Z",
     "start_time": "2024-07-22T22:39:30.243102Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "y_pred_logreg = logistic_regression_model.predict(X_valid)\n",
    "f1_logistic_regression = f1_score(y_valid, y_pred_logreg, average='weighted')\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_valid, y_pred_logreg))\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(f\"Logistic Regression Model F1-Score: {f1_logistic_regression}\")\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "report = classification_report(y_valid, y_pred_logreg, output_dict=True)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_valid, y_pred_logreg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa5e2420a56f852",
   "metadata": {},
   "source": [
    "The Logistic Regression classifier achieved an impressive accuracy of 92.9% on the validation set. The weighted F1-score of 92.3% indicates a strong overall performance. The classifier demonstrated excellent precision and recall for the 'POLITICS' category, with an F1-score of 96%. However, for the 'STYLE' category, while the precision was very high at 99%, the recall was significantly lower at 65%, resulting in an F1-score of 78%. This suggests that while the model is very good at identifying true positives for 'STYLE', it also misses a substantial number of them, indicating room for improvement in balancing the recall across categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37706282f2e2f6f",
   "metadata": {},
   "source": [
    "#### Performance evaluation of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b15f782dcf86a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:30.479232Z",
     "start_time": "2024-07-22T22:39:30.384114Z"
    }
   },
   "outputs": [],
   "source": [
    "random_forest_model = joblib.load('random_forest_without_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47537e07b70d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:30.668988Z",
     "start_time": "2024-07-22T22:39:30.481642Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_rf = random_forest_model.predict(X_valid)\n",
    "f1_logistic_regression = f1_score(y_valid, y_pred_rf, average='weighted')\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_valid, y_pred_rf))\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(f\"Random Forest Model F1-Score: {f1_logistic_regression}\")\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "report = classification_report(y_valid, y_pred_rf, output_dict=True)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_valid, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facf7c4c1b66207b",
   "metadata": {},
   "source": [
    "The Random Forest classifier achieved a high accuracy of 94.5% on the validation set, with a weighted F1-score of 94.2%, indicating robust overall performance. It demonstrated strong precision and recall for the 'POLITICS' category, achieving an F1-score of 97%. For the 'STYLE' category, the model showed improved performance compared to the logistic regression, with an F1-score of 84%, reflecting better balance between precision and recall. This suggests that the Random Forest model is effective in distinguishing between categories and handles class imbalance better than the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666dd822179d217",
   "metadata": {},
   "source": [
    "#### Performance evaluation of deep learning model MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3159ae1c1f4490b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:30.700212Z",
     "start_time": "2024-07-22T22:39:30.668988Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = MLP(input_size, hidden_size, num_classes)\n",
    "model.load_state_dict(torch.load('mlp_model_state.pth'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f53845603c719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:43.605458Z",
     "start_time": "2024-07-22T22:39:30.704660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5c9b8071217f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:43.681703Z",
     "start_time": "2024-07-22T22:39:43.608521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# Calculate F1 score and classification report\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "print(f\"MLP Validation F1 Score: {f1}\")\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "report = classification_report(all_labels, all_predictions)\n",
    "print(\"MLP Validation Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a514b2a9fa67951",
   "metadata": {},
   "source": [
    "The Multilayer Perceptron (MLP) classifier performed exceptionally well on the validation set, achieving an accuracy of 98.0% and a weighted F1-score of 98.1%. The model demonstrated high precision and recall for both classes, with an F1-score of 99% for class 0 ('POLITICS') and 95% for class 1 ('STYLE'). The balanced performance across categories, especially the strong recall for class 0 and high precision for class 1, highlights the MLP's effectiveness in distinguishing between classes and handling class imbalance. Overall, the MLP shows superior performance compared to the logistic regression and Random Forest models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b4ba2bea61b56",
   "metadata": {},
   "source": [
    "### Apply at least one change to the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31717b757d268066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:44.552080Z",
     "start_time": "2024-07-22T22:39:43.683728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_logreg = grid_search.best_estimator_\n",
    "\n",
    "joblib.dump(best_logreg, 'logreg_model_with_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1644469ff119a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:39:44.644964Z",
     "start_time": "2024-07-22T22:39:44.554082Z"
    }
   },
   "outputs": [],
   "source": [
    "best_logreg = joblib.load('logreg_model_with_params.pkl')\n",
    "y_valid_pred = best_logreg.predict(X_valid)\n",
    "valid_accuracy_logreg = accuracy_score(y_valid, y_valid_pred)\n",
    "print(f'Logistic Regression Validation Accuracy: {valid_accuracy_logreg}')\n",
    "\n",
    "# Classification report and F1 score for Logistic Regression\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_valid, y_valid_pred))\n",
    "f1_logreg = f1_score(y_valid, y_valid_pred, average='weighted')\n",
    "print(f'Logistic Regression F1 Score: {f1_logreg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4095e8677673af",
   "metadata": {},
   "source": [
    "To improve the Logistic Regression model, I performed a grid search to optimize hyperparameters, specifically testing different values of C, penalty, and solver. The best parameters found resulted in a validation accuracy of 95.9% and an F1-score of 95.8%, indicating improved performance. The optimized model achieved better precision and recall for both classes, especially enhancing recall for the 'STYLE' category, which was previously lower. This demonstrates the effectiveness of hyperparameter tuning in refining model performance and achieving closer to the benchmark for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af14b99fb8b3b13",
   "metadata": {},
   "source": [
    "### Random Forest with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574444e722cd2cf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:41:53.209600Z",
     "start_time": "2024-07-22T22:39:44.649043Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [10, 20, None]\n",
    "}\n",
    "\n",
    "# Initializing Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initializing GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fitting GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "joblib.dump(best_rf, 'random_forest_with_params.pkl')\n",
    "\n",
    "# Validation predictions and accuracy\n",
    "y_valid_pred = best_rf.predict(X_valid)\n",
    "valid_accuracy_rf = accuracy_score(y_valid, y_valid_pred)\n",
    "print(f'Random Forest Validation Accuracy: {valid_accuracy_rf}')\n",
    "\n",
    "# Classification report and F1 score for Random Forest\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_valid, y_valid_pred))\n",
    "f1_rf = f1_score(y_valid, y_valid_pred, average='weighted')\n",
    "print(f'Random Forest F1 Score: {f1_rf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c8a616803982b",
   "metadata": {},
   "source": [
    "By tuning the Random Forest model with GridSearchCV, I optimized hyperparameters such as n_estimators, max_features, and max_depth, resulting in a validation accuracy of 95.2% and a weighted F1-score of 94.9%. This improvement is evident in the enhanced recall for the 'STYLE' category and consistently high precision for 'POLITICS'. The refined model shows better performance and robustness, addressing class imbalance issues more effectively and coming closer to the benchmark expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802f47052fc29ad",
   "metadata": {},
   "source": [
    "### MlP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d244fc27d268958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:04.016857Z",
     "start_time": "2024-07-22T22:41:53.209600Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_dl = train_df.copy()\n",
    "valid_df_dl = val_df.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "train_df_dl['category'] = label_encoder.fit_transform(train_df_dl['category'])\n",
    "valid_df_dl['category'] = label_encoder.transform(valid_df_dl['category'])\n",
    "# Target variable\n",
    "y_train_dl = train_df_dl['category'].values\n",
    "y_valid_dl = valid_df_dl['category'].values\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=4500, stop_words='english')\n",
    "\n",
    "X_train_dl = vectorizer.fit_transform(train_df_dl['text']).toarray()\n",
    "\n",
    "X_valid_dl = vectorizer.transform(valid_df_dl['text']).toarray()\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "input_size = X_train_dl.shape[1]\n",
    "hidden_size = 200  # Changed hidden size\n",
    "num_classes = len(y_train.unique())\n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "num_epochs = 10  \n",
    "batch_size = 64 # Changed batch size\n",
    "\n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in valid_loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "print(f\"MLP Validation F1 Score: {f1}\")\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "report = classification_report(all_labels, all_predictions)\n",
    "print(\"MLP Validation Classification Report:\")\n",
    "print(report)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f037bda1d0e9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:04.032481Z",
     "start_time": "2024-07-22T22:42:04.016857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'mlp_model_with_params.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d387fbbf513cb8e5",
   "metadata": {},
   "source": [
    "I made several adjustments to the MLP model, including increasing the hidden layer size to 200. The model achieved a validation F1-score of 98.1%, with high precision and recall for both classes, particularly excelling in classifying 'POLITICS' with near-perfect recall. These changes resulted in enhanced performance, demonstrating the model's capacity to better learn and generalize from the data, achieving strong results across evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433e14f656703f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:04.064161Z",
     "start_time": "2024-07-22T22:42:04.033969Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combining the training and validation data\n",
    "combined_df = pd.concat([train_df, valid_df])\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8199011ce54aa97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:04.235244Z",
     "start_time": "2024-07-22T22:42:04.064924Z"
    }
   },
   "outputs": [],
   "source": [
    "X_combined = vectorizer.transform(combined_df['text'])\n",
    "y_combined = combined_df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77a4ff25eeb976",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:04.249185Z",
     "start_time": "2024-07-22T22:42:04.235244Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25cfc1764a93882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:04.271037Z",
     "start_time": "2024-07-22T22:42:04.249892Z"
    }
   },
   "outputs": [],
   "source": [
    "best_logreg = joblib.load('logreg_model_with_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb2b217a343bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:04.422631Z",
     "start_time": "2024-07-22T22:42:04.271037Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_scores_logreg = cross_val_score(best_logreg, X_combined, y_combined, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(\"Logistic Regression Cross-Validation Scores:\", cv_scores_logreg)\n",
    "print(\"Logistic Regression Mean Cross-Validation Accuracy:\", cv_scores_logreg.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c0e072d4e0bf",
   "metadata": {},
   "source": [
    "The cross-validation scores for the Logistic Regression model indicate consistent and high performance across different folds, with accuracies ranging from 94.9% to 96.5%. The mean cross-validation accuracy is 95.9%, reflecting the model's robustness and generalizability. These results confirm that the optimized Logistic Regression model performs well across diverse subsets of the combined dataset, demonstrating its effectiveness in handling the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0f4ead9087f5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:20.839453Z",
     "start_time": "2024-07-22T22:42:04.422631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cv_scores_rf = cross_val_score(best_rf, X_combined, y_combined, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(\"Random Forest Cross-Validation Scores:\", cv_scores_rf)\n",
    "print(\"Random Forest Mean Cross-Validation Accuracy:\", cv_scores_rf.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a24085ba0e678",
   "metadata": {},
   "source": [
    "The Random Forest model's cross-validation results show high and stable performance, with accuracies ranging from 95.2% to 96.6%. The mean cross-validation accuracy of 96.1% indicates strong generalization across different subsets of the data. These results highlight the Random Forest model's robustness and effectiveness, confirming its reliability for the classification task. The consistent high performance across folds suggests that the model is well-tuned and capable of maintaining accuracy on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d117445226a01b7",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8741a1857a0e13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:21.078812Z",
     "start_time": "2024-07-22T22:42:20.839453Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df_dl = combined_df.copy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_df_dl['category'] = label_encoder.fit_transform(train_df_dl['category'])\n",
    "\n",
    "# Target variable\n",
    "y_train_dl = train_df_dl['category'].values\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=4500, stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_dl = vectorizer.fit_transform(train_df_dl['text']).toarray()\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_dl, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_dl, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c97a8f389c525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:36.179699Z",
     "start_time": "2024-07-22T22:42:21.078812Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_mlp = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation phase\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in valid_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "        all_labels.extend(batch_y.numpy())\n",
    "\n",
    "# Computing metrics\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "print(f\"MLP Validation F1 Score: {f1}\")\n",
    "report = classification_report(all_labels, all_predictions)\n",
    "print(\"MLP Validation Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "cv_scores_mlp.append(f1)\n",
    "\n",
    "print(\"MLP Cross-Validation F1 Scores:\", cv_scores_mlp)\n",
    "print(\"MLP Mean Cross-Validation F1 Score:\", np.mean(cv_scores_mlp))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e248cc6bcfec1db",
   "metadata": {},
   "source": [
    "The MLP model achieved a validation F1-score of 93.2%, with high precision and recall for the 'POLITICS' category but lower performance for the 'STYLE' category. The mean cross-validation F1-score matches the validation F1-score, indicating consistent performance across different folds. The results show that while the MLP is effective in classifying 'POLITICS', it struggles more with 'STYLE', suggesting that further tuning or adjustments might be needed to balance performance across all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3df6148b1e10691",
   "metadata": {},
   "source": [
    "The Random Forest model performed the best using cross-validation, with a mean accuracy of 96.1% and consistent high scores across folds. This indicates its robustness and superior generalization compared to the other models. The Logistic Regression and MLP models also performed well but did not match the Random Forest’s accuracy, highlighting the Random Forest's strength in handling the classification task with high reliability and consistency. So we will use saved Random Forest for our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e63f781e5c97110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:36.211213Z",
     "start_time": "2024-07-22T22:42:36.180337Z"
    }
   },
   "outputs": [],
   "source": [
    "# 13\n",
    "test = pd.read_csv('test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e07b1ddef9219c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:36.399301Z",
     "start_time": "2024-07-22T22:42:36.211213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting words to numerical representation\n",
    "vectorizer = TfidfVectorizer(max_features=4500, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(train_df['text']) #Just a reference\n",
    "\n",
    "# Transforming the test data\n",
    "X_test = vectorizer.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc716e124e6acab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:36.415209Z",
     "start_time": "2024-07-22T22:42:36.399465Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test = test_df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e0507aa2feaa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:36.571788Z",
     "start_time": "2024-07-22T22:42:36.415209Z"
    }
   },
   "outputs": [],
   "source": [
    "best_rf = joblib.load('random_forest_with_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c972934f2ba122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:36.787060Z",
     "start_time": "2024-07-22T22:42:36.571788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making predictions on the test set\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_random_forest = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "# Print Accuracy and F1-Score\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(f\"Random Forest Model F1-Score: {f1_random_forest}\")\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "\n",
    "# Print Classification Report\n",
    "report = classification_report(y_test,y_pred_rf)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c900e6197dcac51b",
   "metadata": {},
   "source": [
    "The Random Forest model achieved an impressive accuracy of 97.6% and an F1-score of 97.6% on the test set, demonstrating excellent overall performance. It performed exceptionally well on 'POLITICS' with high precision and recall, and also showed strong results for 'STYLE', with an F1-score of 94%. These results highlight the model's effectiveness and reliability in classifying both categories accurately, confirming its robustness and suitability for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c2ddbcb2a8ee1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:36.818333Z",
     "start_time": "2024-07-22T22:42:36.787060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combining the training and validation data\n",
    "combined_df = pd.concat([train_df, valid_df])\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973140dfa2b3b2c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:36.943713Z",
     "start_time": "2024-07-22T22:42:36.818333Z"
    }
   },
   "outputs": [],
   "source": [
    "X_combined = vectorizer.transform(combined_df['text'])\n",
    "y_combined = combined_df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceedcc4669503032",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:37.167080Z",
     "start_time": "2024-07-22T22:42:36.943713Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = joblib.load('random_forest_with_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101da645348e984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T22:42:42.211502Z",
     "start_time": "2024-07-22T22:42:37.167080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing Random Forest model\n",
    "best_model.fit(X_combined, y_combined)\n",
    "\n",
    "y_pred_rf = best_model.predict(X_test)\n",
    "\n",
    "f1_random_forest = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(f\"Random Forest Model F1-Score: {f1_random_forest}\")\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "\n",
    "report = classification_report(y_test, y_pred_rf)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df0eda8e30b566b",
   "metadata": {},
   "source": [
    "Retraining the Random Forest model with both the train and validation datasets resulted in a test accuracy of 96.3% and an F1-score of 96.3%. These metrics are slightly lower compared to the model trained solely on the training set, which achieved 97.6% accuracy and a 97.6% F1-score. This decrease suggests that including the validation data for retraining led to a slight reduction in performance, potentially due to the model's exposure to a larger, more diverse dataset. Despite this, the retrained model still shows strong performance, confirming its effectiveness in handling the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494093e8cec0486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
